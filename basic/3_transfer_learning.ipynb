{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3ef7f83",
   "metadata": {},
   "source": [
    "# Level 3: Transfer Learning\n",
    "## Use pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3168461",
   "metadata": {},
   "source": [
    "https://lightning.ai/docs/pytorch/stable/advanced/transfer_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3f63ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lightning import LightningModule\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f980ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data sets\n",
    "transform = transforms.ToTensor()\n",
    "train_set = datasets.MNIST(root=\"../data/MNIST\", download=True, train=True, transform=transform)\n",
    "test_set = datasets.MNIST(root=\"../data/MNIST\", download=True, train=False, transform=transform)\n",
    "\n",
    "# use 20% of training data for validation\n",
    "train_set_size = int(len(train_set) * 0.8)\n",
    "valid_set_size = len(train_set) - train_set_size\n",
    "\n",
    "# split the train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=16, persistent_workers=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=128, num_workers=16, persistent_workers=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=1024, num_workers=16, persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa6583",
   "metadata": {},
   "source": [
    "### Pretraining phase\n",
    "Define the encoder and the decoder. In the Pretraining phase we train the encoder to recreate a significant representation of the image data. This is done in such a way that a decoder is able to recosntruct the full image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b550dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim=28*28, hidden_nodes_1=64, hidden_nodes_2=64, out_dim=4):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_nodes_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_nodes_1, hidden_nodes_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_nodes_2, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim=4, hidden_nodes_1=64, hidden_nodes_2=64, out_dim=28*28):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_nodes_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_nodes_1, hidden_nodes_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_nodes_2, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "class LitAutoEncoder(LightningModule):\n",
    "    def __init__(self, encoder, decoder, lr=1e-5):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters(ignore=[\"encoder\", \"decoder\"])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        loss = self._get_loss(batch)\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        loss = self._get_loss(batch)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        loss = self._get_loss(batch)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "    def _get_loss(self, batch):\n",
    "        x, _ = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_hat = self.forward(x)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "708d722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitAutoEncoder(\n",
    "    encoder=Encoder(\n",
    "        in_dim=28*28,\n",
    "        hidden_nodes_1=512,\n",
    "        hidden_nodes_2=256,\n",
    "        out_dim=100\n",
    "    ),\n",
    "    decoder=Decoder(\n",
    "        in_dim=100,\n",
    "        hidden_nodes_1=128,\n",
    "        hidden_nodes_2=256,\n",
    "        out_dim=28*28\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff470c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\n",
    "    save_dir='logs',\n",
    "    name='autoencoder_mnist',\n",
    "    version=None,\n",
    "    prefix='test_'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(logger.log_dir, \"checkpoints\"),\n",
    "    filename=\"autoencoder_best-{epoch:02d}-{val_loss:.3f}\",\n",
    "    monitor=\"val_loss\",    \n",
    "    mode=\"min\",\n",
    "    save_top_k=3,     # keep ONLY the best\n",
    "    save_last=True    # ALSO save last.ckpt\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be1ff9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage3/DSIP/rriva/tutorials/pl_tutorial/.venv/lib/python3.13/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /storage3/DSIP/rriva/tutorials/pl_tutorial/.venv/lib ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc198f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA L40S') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name    | Type    | Params | Mode  | FLOPs\n",
      "----------------------------------------------------\n",
      "0 | encoder | Encoder | 558 K  | train | 0    \n",
      "1 | decoder | Decoder | 247 K  | train | 0    \n",
      "----------------------------------------------------\n",
      "806 K     Trainable params\n",
      "0         Non-trainable params\n",
      "806 K     Total params\n",
      "3.226     Total estimated model params size (MB)\n",
      "14        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 928: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:03<00:00, 106.33it/s, v_num=0, val_loss=0.00538, train_loss=0.00536]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117d9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks.early_stopping import EarlyStoppingReason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "001bf20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped due to patience exhaustion\n"
     ]
    }
   ],
   "source": [
    "# Check why training stopped\n",
    "if early_stop_callback.stopping_reason == EarlyStoppingReason.PATIENCE_EXHAUSTED:\n",
    "    print(\"Training stopped due to patience exhaustion\")\n",
    "elif early_stop_callback.stopping_reason == EarlyStoppingReason.STOPPING_THRESHOLD:\n",
    "    print(\"Training stopped due to reaching stopping threshold\")\n",
    "elif early_stop_callback.stopping_reason == EarlyStoppingReason.NOT_STOPPED:\n",
    "    print(\"Training completed normally without early stopping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b27c2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details: Monitored metric val_loss did not improve in the last 3 records. Best score: 0.005. Signaling Trainer to stop.\n"
     ]
    }
   ],
   "source": [
    "# Access human-readable message\n",
    "if early_stop_callback.stopping_reason_message:\n",
    "    print(f\"Details: {early_stop_callback.stopping_reason_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159719bf",
   "metadata": {},
   "source": [
    "### Training Phase\n",
    "Now we define the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46292f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model checkpoint path: /storage3/DSIP/rriva/tutorials/pl_tutorial/basic/logs/autoencoder_mnist/version_0/checkpoints/autoencoder_best-epoch=925-val_loss=0.005.ckpt\n"
     ]
    }
   ],
   "source": [
    "AutoEncoder_checkpoint_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best model checkpoint path: {AutoEncoder_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc07f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(LightningModule):\n",
    "    def __init__(self, checkpoint_path,encoder,decoder):\n",
    "        super().__init__()\n",
    "        # init the pretrained LightningModule\n",
    "        autoencoder = LitAutoEncoder.load_from_checkpoint(\n",
    "            checkpoint_path,\n",
    "            encoder=encoder,\n",
    "            decoder=decoder\n",
    "        )\n",
    "        self.feature_extractor = autoencoder.encoder\n",
    "        #self.feature_extractor.freeze()\n",
    "        self.feature_extractor.requires_grad_(False)\n",
    "        self.lr = autoencoder.lr\n",
    "        self.save_hyperparameters(ignore=[\"feature_extractor\"])\n",
    "        \n",
    "        # the autoencoder outputs a 100-dim representation and CIFAR-10 has 10 classes\n",
    "        self.classifier = nn.Linear(encoder.ff[-1].out_features, 10)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        loss = self._get_loss(batch)\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        loss = self._get_loss(batch)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        loss = self._get_loss(batch)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "    def _get_loss(self, batch):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y_hat = self.forward(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        representations = self.feature_extractor(x)\n",
    "        x = self.classifier(representations)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f1be51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = MNISTClassifier(AutoEncoder_checkpoint_path,\n",
    "    encoder=Encoder(\n",
    "        in_dim=28*28,\n",
    "        hidden_nodes_1=512,\n",
    "        hidden_nodes_2=256,\n",
    "        out_dim=100\n",
    "    ),\n",
    "    decoder=Decoder(\n",
    "        in_dim=100,\n",
    "        hidden_nodes_1=128,\n",
    "        hidden_nodes_2=256,\n",
    "        out_dim=28*28\n",
    "    )                                \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8cd5da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\n",
    "    save_dir='logs',\n",
    "    name='classifier_mnist',\n",
    "    version=None\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(logger.log_dir, \"checkpoints\"),\n",
    "    filename=\"classifier-{epoch:02d}-{val_loss:.3f}\",\n",
    "    monitor=\"val_loss\",    \n",
    "    mode=\"min\",\n",
    "    save_top_k=3,     # keep ONLY the best\n",
    "    save_last=True    # ALSO save last.ckpt\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "222f9c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage3/DSIP/rriva/tutorials/pl_tutorial/.venv/lib/python3.13/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /storage3/DSIP/rriva/tutorials/pl_tutorial/.venv/lib ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "classifier_trainer = Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4551d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name              | Type    | Params | Mode  | FLOPs\n",
      "--------------------------------------------------------------\n",
      "0 | feature_extractor | Encoder | 558 K  | train | 0    \n",
      "1 | classifier        | Linear  | 1.0 K  | train | 0    \n",
      "--------------------------------------------------------------\n",
      "1.0 K     Trainable params\n",
      "558 K     Non-trainable params\n",
      "559 K     Total params\n",
      "2.240     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 494:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 257/375 [00:01<00:00, 143.71it/s, v_num=3, val_loss=0.384, train_loss=0.363]"
     ]
    }
   ],
   "source": [
    "classifier_trainer.fit(classifier_model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d9141f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'early_stop_callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Access human-readable message\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mearly_stop_callback\u001b[49m.stopping_reason_message:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDetails: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mearly_stop_callback.stopping_reason_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'early_stop_callback' is not defined"
     ]
    }
   ],
   "source": [
    "# Access human-readable message\n",
    "if early_stop_callback.stopping_reason_message:\n",
    "    print(f\"Details: {early_stop_callback.stopping_reason_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c83bc2e",
   "metadata": {},
   "source": [
    "## Automated Finetuning with Callbacks\n",
    "\n",
    "PyTorch Lightning provides the BackboneFinetuning callback to automate the finetuning process. This callback gradually unfreezes your modelâ€™s backbone during training. This is particularly useful when working with large pretrained models, as it allows you to start training with a frozen backbone and then progressively unfreeze layers to fine-tune the model.\n",
    "\n",
    "The `BackboneFinetuning` callback expects your model to have a specific structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc61c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # REQUIRED: Your model must have a 'backbone' attribute\n",
    "        # This should be the pretrained part you want to finetune\n",
    "        self.backbone = some_pretrained_model\n",
    "\n",
    "        # Your task-specific layers (head, classifier, etc.)\n",
    "        self.head = nn.Linear(backbone_features, num_classes)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Only optimize the head initially - backbone will be added automatically\n",
    "        return torch.optim.Adam(self.head.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1acdd98",
   "metadata": {},
   "source": [
    "### Example: Computer Vision with ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21508db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 8 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=8)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from lightning.pytorch import LightningModule, Trainer\n",
    "from lightning.pytorch.callbacks import BackboneFinetuning\n",
    "\n",
    "\n",
    "class ResNetClassifier(LightningModule):\n",
    "    def __init__(self, num_classes=10, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Create backbone from pretrained ResNet\n",
    "        resnet = models.resnet50(weights=\"DEFAULT\")\n",
    "        # Remove the final classification layer\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        # Add custom classification head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(resnet.fc.in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features with backbone\n",
    "        features = self.backbone(x)\n",
    "        # Classify with head\n",
    "        return self.head(features)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Initially only train the head - backbone will be added by callback\n",
    "        return torch.optim.Adam(self.head.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "\n",
    "# Setup the finetuning callback\n",
    "backbone_finetuning = BackboneFinetuning(\n",
    "    unfreeze_backbone_at_epoch=10,  # Start unfreezing backbone at epoch 10\n",
    "    lambda_func=lambda epoch: 1.5,  # Gradually increase backbone learning rate\n",
    "    backbone_initial_ratio_lr=0.1,  # Backbone starts at 10% of head learning rate\n",
    "    should_align=True,  # Align rates when backbone rate reaches head rate\n",
    "    verbose=True  # Print learning rates during training\n",
    ")\n",
    "\n",
    "model = ResNetClassifier()\n",
    "trainer = Trainer(callbacks=[backbone_finetuning], max_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2b40a7",
   "metadata": {},
   "source": [
    "### Custom Finetuning Strategies\n",
    "For more control, you can create custom finetuning strategies by subclassing `BaseFinetuning`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a03024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks.finetuning import BaseFinetuning\n",
    "b\n",
    "class CustomFinetuning(BaseFinetuning):\n",
    "    def __init__(self, unfreeze_at_epoch=5, layers_per_epoch=2):\n",
    "        super().__init__()\n",
    "        self.unfreeze_at_epoch = unfreeze_at_epoch\n",
    "        self.layers_per_epoch = layers_per_epoch\n",
    "\n",
    "    def freeze_before_training(self, pl_module):\n",
    "        # Freeze the entire backbone initially\n",
    "        self.freeze(pl_module.backbone)\n",
    "\n",
    "    def finetune_function(self, pl_module, epoch, optimizer):\n",
    "        # Gradually unfreeze layers\n",
    "        if epoch >= self.unfreeze_at_epoch:\n",
    "            layers_to_unfreeze = min(\n",
    "                self.layers_per_epoch,\n",
    "                len(list(pl_module.backbone.children()))\n",
    "            )\n",
    "\n",
    "            # Unfreeze from the top layers down\n",
    "            backbone_children = list(pl_module.backbone.children())\n",
    "            for layer in backbone_children[-layers_to_unfreeze:]:\n",
    "                self.unfreeze_and_add_param_group(\n",
    "                    layer, optimizer, lr=1e-4\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
