{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3ef7f83",
   "metadata": {},
   "source": [
    "# Level 2: Add a validation and test set\n",
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3168461",
   "metadata": {},
   "source": [
    "https://lightning.ai/docs/pytorch/stable/common/early_stopping.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d3f63ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lightning import LightningModule\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f980ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data sets\n",
    "transform = transforms.ToTensor()\n",
    "train_set = datasets.MNIST(root=\"../data/MNIST\", download=True, train=True, transform=transform)\n",
    "test_set = datasets.MNIST(root=\"../data/MNIST\", download=True, train=False, transform=transform)\n",
    "\n",
    "# use 20% of training data for validation\n",
    "train_set_size = int(len(train_set) * 0.8)\n",
    "valid_set_size = len(train_set) - train_set_size\n",
    "\n",
    "# split the train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=16, persistent_workers=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=128, num_workers=16, persistent_workers=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=1024, num_workers=16, persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40199c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function log in module lightning.pytorch.core.module:\n",
      "\n",
      "log(\n",
      "    self,\n",
      "    name: str,\n",
      "    value: Union[torchmetrics.metric.Metric, torch.Tensor, int, float],\n",
      "    prog_bar: bool = False,\n",
      "    logger: Optional[bool] = None,\n",
      "    on_step: Optional[bool] = None,\n",
      "    on_epoch: Optional[bool] = None,\n",
      "    reduce_fx: Union[str, Callable[[Any], Any]] = 'mean',\n",
      "    enable_graph: bool = False,\n",
      "    sync_dist: bool = False,\n",
      "    sync_dist_group: Optional[Any] = None,\n",
      "    add_dataloader_idx: bool = True,\n",
      "    batch_size: Optional[int] = None,\n",
      "    metric_attribute: Optional[str] = None,\n",
      "    rank_zero_only: bool = False\n",
      ") -> None\n",
      "    Log a key, value pair.\n",
      "\n",
      "    Example::\n",
      "\n",
      "        self.log('train_loss', loss)\n",
      "\n",
      "    The default behavior per hook is documented here: :ref:`extensions/logging:Automatic Logging`.\n",
      "\n",
      "    Args:\n",
      "        name: key to log. Must be identical across all processes if using DDP or any other distributed strategy.\n",
      "        value: value to log. Can be a ``float``, ``Tensor``, or a ``Metric``.\n",
      "        prog_bar: if ``True`` logs to the progress bar.\n",
      "        logger: if ``True`` logs to the logger.\n",
      "        on_step: if ``True`` logs at this step. The default value is determined by the hook.\n",
      "            See :ref:`extensions/logging:Automatic Logging` for details.\n",
      "        on_epoch: if ``True`` logs epoch accumulated metrics. The default value is determined by the hook.\n",
      "            See :ref:`extensions/logging:Automatic Logging` for details.\n",
      "        reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default.\n",
      "        enable_graph: if ``True``, will not auto detach the graph.\n",
      "        sync_dist: if ``True``, reduces the metric across devices. Use with care as this may lead to a significant\n",
      "            communication overhead.\n",
      "        sync_dist_group: the DDP group to sync across.\n",
      "        add_dataloader_idx: if ``True``, appends the index of the current dataloader to\n",
      "            the name (when using multiple dataloaders). If False, user needs to give unique names for\n",
      "            each dataloader to not mix the values.\n",
      "        batch_size: Current batch_size. This will be directly inferred from the loaded batch,\n",
      "            but for some data structures you might need to explicitly provide it.\n",
      "        metric_attribute: To restore the metric state, Lightning requires the reference of the\n",
      "            :class:`torchmetrics.Metric` in your model. This is found automatically if it is a model attribute.\n",
      "        rank_zero_only: Tells Lightning if you are calling ``self.log`` from every process (default) or only from\n",
      "            rank 0. If ``True``, you won't be able to use this metric as a monitor in callbacks\n",
      "            (e.g., early stopping). Warning: Improper use can lead to deadlocks! See\n",
      "            :ref:`Advanced Logging <visualize/logging_advanced:rank_zero_only>` for more details.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LightningModule.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b550dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim=28*28, hidden_nodes_1=64, hidden_nodes_2=64, out_dim=4):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_nodes_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_nodes_1, hidden_nodes_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_nodes_2, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim=4, hidden_nodes_1=64, hidden_nodes_2=64, out_dim=28*28):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_nodes_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_nodes_1, hidden_nodes_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_nodes_2, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "class LitAutoEncoder(LightningModule):\n",
    "    def __init__(self, encoder, decoder, lr=1e-5):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters(ignore=[\"encoder\", \"decoder\"])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        loss = self._get_loss(batch)\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        loss = self._get_loss(batch)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        loss = self._get_loss(batch)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "    def _get_loss(self, batch):\n",
    "        x, _ = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_hat = self.forward(x)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "708d722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitAutoEncoder(\n",
    "    encoder=Encoder(),\n",
    "    decoder=Decoder()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51711701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CSVLogger in module lightning.pytorch.loggers.csv_logs:\n",
      "\n",
      "class CSVLogger(lightning.pytorch.loggers.logger.Logger, lightning.fabric.loggers.csv_logs.CSVLogger)\n",
      " |  CSVLogger(\n",
      " |      save_dir: Union[str, pathlib._local.Path],\n",
      " |      name: Optional[str] = 'lightning_logs',\n",
      " |      version: Union[int, str, NoneType] = None,\n",
      " |      prefix: str = '',\n",
      " |      flush_logs_every_n_steps: int = 100\n",
      " |  )\n",
      " |\n",
      " |  Log to local file system in yaml and CSV format.\n",
      " |\n",
      " |  Logs are saved to ``os.path.join(save_dir, name, version)``.\n",
      " |\n",
      " |  Example:\n",
      " |      >>> from lightning.pytorch import Trainer\n",
      " |      >>> from lightning.pytorch.loggers import CSVLogger\n",
      " |      >>> logger = CSVLogger(\"logs\", name=\"my_exp_name\")\n",
      " |      >>> trainer = Trainer(logger=logger)\n",
      " |\n",
      " |  Args:\n",
      " |      save_dir: Save directory\n",
      " |      name: Experiment name, optional. Defaults to ``'lightning_logs'``. If name is ``None``, logs\n",
      " |          (versions) will be stored to the save dir directly.\n",
      " |      version: Experiment version. If version is not specified the logger inspects the save\n",
      " |          directory for existing versions, then automatically assigns the next available version.\n",
      " |      prefix: A string to put at the beginning of metric keys.\n",
      " |      flush_logs_every_n_steps: How often to flush logs to disk (defaults to every 100 steps).\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      CSVLogger\n",
      " |      lightning.pytorch.loggers.logger.Logger\n",
      " |      lightning.fabric.loggers.csv_logs.CSVLogger\n",
      " |      lightning.fabric.loggers.logger.Logger\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(\n",
      " |      self,\n",
      " |      save_dir: Union[str, pathlib._local.Path],\n",
      " |      name: Optional[str] = 'lightning_logs',\n",
      " |      version: Union[int, str, NoneType] = None,\n",
      " |      prefix: str = '',\n",
      " |      flush_logs_every_n_steps: int = 100\n",
      " |  )\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  log_hyperparams(\n",
      " |      self,\n",
      " |      params: Union[dict[str, Any], argparse.Namespace, NoneType] = None\n",
      " |  ) -> None\n",
      " |      Record hyperparameters.\n",
      " |\n",
      " |      Args:\n",
      " |          params: :class:`~argparse.Namespace` or `Dict` containing the hyperparameters\n",
      " |          args: Optional positional arguments, depends on the specific logger being used\n",
      " |          kwargs: Optional keyword arguments, depends on the specific logger being used\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  experiment\n",
      " |      Actual _ExperimentWriter object. To use _ExperimentWriter features in your\n",
      " |      :class:`~lightning.pytorch.core.LightningModule` do the following.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          self.logger.experiment.some_experiment_writer_function()\n",
      " |\n",
      " |  log_dir\n",
      " |      The log directory for this run.\n",
      " |\n",
      " |      By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\n",
      " |      constructor's version parameter instead of ``None`` or an int.\n",
      " |\n",
      " |  root_dir\n",
      " |      Parent directory for all checkpoint subdirectories.\n",
      " |\n",
      " |      If the experiment name parameter is an empty string, no experiment subdirectory is used and the checkpoint will\n",
      " |      be saved in \"save_dir/version\"\n",
      " |\n",
      " |  save_dir\n",
      " |      The current directory where logs are saved.\n",
      " |\n",
      " |      Returns:\n",
      " |          The path to current directory where logs are saved.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  LOGGER_JOIN_CHAR = '-'\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from lightning.pytorch.loggers.logger.Logger:\n",
      " |\n",
      " |  after_save_checkpoint(\n",
      " |      self,\n",
      " |      checkpoint_callback: lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint\n",
      " |  ) -> None\n",
      " |      Called after model checkpoint callback saves a new checkpoint.\n",
      " |\n",
      " |      Args:\n",
      " |          checkpoint_callback: the model checkpoint callback instance\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from lightning.fabric.loggers.csv_logs.CSVLogger:\n",
      " |\n",
      " |  finalize(self, status: str) -> None\n",
      " |      Do any processing that is necessary to finalize an experiment.\n",
      " |\n",
      " |      Args:\n",
      " |          status: Status that the experiment finished with (e.g. success, failed, aborted)\n",
      " |\n",
      " |  log_metrics(\n",
      " |      self,\n",
      " |      metrics: dict[str, typing.Union[torch.Tensor, float]],\n",
      " |      step: Optional[int] = None\n",
      " |  ) -> None\n",
      " |      Records metrics. This method logs metrics as soon as it received them.\n",
      " |\n",
      " |      Args:\n",
      " |          metrics: Dictionary with metric names as keys and measured quantities as values\n",
      " |          step: Step number at which the metrics should be recorded\n",
      " |\n",
      " |  save(self) -> None\n",
      " |      Save log data.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from lightning.fabric.loggers.csv_logs.CSVLogger:\n",
      " |\n",
      " |  name\n",
      " |      Gets the name of the experiment.\n",
      " |\n",
      " |      Returns:\n",
      " |          The name of the experiment.\n",
      " |\n",
      " |  version\n",
      " |      Gets the version of the experiment.\n",
      " |\n",
      " |      Returns:\n",
      " |          The version of the experiment if it is specified, else the next version.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from lightning.fabric.loggers.logger.Logger:\n",
      " |\n",
      " |  log_graph(\n",
      " |      self,\n",
      " |      model: torch.nn.modules.module.Module,\n",
      " |      input_array: Optional[torch.Tensor] = None\n",
      " |  ) -> None\n",
      " |      Record model graph.\n",
      " |\n",
      " |      Args:\n",
      " |          model: the model with an implementation of ``forward``.\n",
      " |          input_array: input passes to `model.forward`\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from lightning.fabric.loggers.logger.Logger:\n",
      " |\n",
      " |  group_separator\n",
      " |      Return the default separator used by the logger to group the data into subfolders.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from lightning.fabric.loggers.logger.Logger:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CSVLogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01cc00be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ModelCheckpoint in module lightning.pytorch.callbacks.model_checkpoint:\n",
      "\n",
      "class ModelCheckpoint(lightning.pytorch.callbacks.checkpoint.Checkpoint)\n",
      " |  ModelCheckpoint(\n",
      " |      dirpath: Union[str, pathlib._local.Path, NoneType] = None,\n",
      " |      filename: Optional[str] = None,\n",
      " |      monitor: Optional[str] = None,\n",
      " |      verbose: bool = False,\n",
      " |      save_last: Union[bool, Literal['link'], NoneType] = None,\n",
      " |      save_top_k: int = 1,\n",
      " |      save_on_exception: bool = False,\n",
      " |      save_weights_only: bool = False,\n",
      " |      mode: str = 'min',\n",
      " |      auto_insert_metric_name: bool = True,\n",
      " |      every_n_train_steps: Optional[int] = None,\n",
      " |      train_time_interval: Optional[datetime.timedelta] = None,\n",
      " |      every_n_epochs: Optional[int] = None,\n",
      " |      save_on_train_epoch_end: Optional[bool] = None,\n",
      " |      enable_version_counter: bool = True\n",
      " |  )\n",
      " |\n",
      " |  Save the model after every epoch by monitoring a quantity. Every logged metrics are passed to the\n",
      " |  :class:`~lightning.pytorch.loggers.logger.Logger` for the version it gets saved in the same directory as the\n",
      " |  checkpoint.\n",
      " |\n",
      " |  After training finishes, use :attr:`best_model_path` to retrieve the path to the\n",
      " |  best checkpoint file and :attr:`best_model_score` to get its score.\n",
      " |\n",
      " |  .. note::\n",
      " |      When using manual optimization with ``every_n_train_steps``, you should save the model state\n",
      " |      in your ``training_step`` before the optimizer step if you want the checkpoint to reflect\n",
      " |      the pre-optimization state. Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          def training_step(self, batch, batch_idx):\n",
      " |              # ... forward pass, loss calculation, backward pass ...\n",
      " |\n",
      " |              # Save model state before optimization\n",
      " |              if not hasattr(self, 'saved_models'):\n",
      " |                  self.saved_models = {}\n",
      " |              self.saved_models[batch_idx] = {\n",
      " |                  k: v.detach().clone()\n",
      " |                  for k, v in self.layer.state_dict().items()\n",
      " |              }\n",
      " |\n",
      " |              # Then perform optimization\n",
      " |              optimizer.zero_grad()\n",
      " |              self.manual_backward(loss)\n",
      " |              optimizer.step()\n",
      " |\n",
      " |              # Optional: Clean up old states to save memory\n",
      " |              if batch_idx > 10:  # Keep last 10 states\n",
      " |                  del self.saved_models[batch_idx - 10]\n",
      " |\n",
      " |  Args:\n",
      " |      dirpath: Directory to save the model file.\n",
      " |          Example: ``dirpath='my/path/'``.\n",
      " |\n",
      " |          .. warning::\n",
      " |              In a distributed environment like DDP, it's recommended to provide a `dirpath` to avoid race conditions.\n",
      " |              When using manual optimization with ``every_n_train_steps``, make sure to save the model state\n",
      " |              in your training loop as shown in the example above.\n",
      " |\n",
      " |          Can be remote file paths such as `s3://mybucket/path/` or 'hdfs://path/'\n",
      " |          (default: ``None``). If dirpath is ``None``, we only keep the ``k`` best checkpoints\n",
      " |          in memory, and do not save anything to disk.\n",
      " |\n",
      " |      filename: Checkpoint filename. Can contain named formatting options to be auto-filled.\n",
      " |          If no name is provided, it will be ``None`` and the checkpoint will be saved to\n",
      " |          ``{epoch}``.and if the Trainer uses a logger, the path will also contain logger name and version.\n",
      " |\n",
      " |      filename: checkpoint filename. Can contain named formatting options to be auto-filled.\n",
      " |\n",
      " |          Example::\n",
      " |\n",
      " |              # save any arbitrary metrics like `val_loss`, etc. in name\n",
      " |              # saves a file like: my/path/epoch=2-val_loss=0.02-other_metric=0.03.ckpt\n",
      " |              >>> checkpoint_callback = ModelCheckpoint(\n",
      " |              ...     dirpath='my/path',\n",
      " |              ...     filename='{epoch}-{val_loss:.2f}-{other_metric:.2f}'\n",
      " |              ... )\n",
      " |\n",
      " |          By default, filename is ``None`` and will be set to ``'{epoch}-{step}'``, where \"epoch\" and \"step\" match\n",
      " |          the number of finished epoch and optimizer steps respectively.\n",
      " |      monitor: quantity to monitor. By default it is ``None`` which saves a checkpoint only for the last epoch.\n",
      " |      verbose: verbosity mode. Default: ``False``.\n",
      " |      save_last: When ``True``, saves a `last.ckpt` copy whenever a checkpoint file gets saved. Can be set to\n",
      " |          ``'link'`` on a local filesystem to create a symbolic link. This allows accessing the latest checkpoint\n",
      " |          in a deterministic manner. Default: ``None``.\n",
      " |      save_top_k: if ``save_top_k == k``,\n",
      " |          the best k models according to the quantity monitored will be saved.\n",
      " |          If ``save_top_k == 0``, no models are saved.\n",
      " |          If ``save_top_k == -1``, all models are saved.\n",
      " |          Please note that the monitors are checked every ``every_n_epochs`` epochs.\n",
      " |          If ``save_top_k >= 2`` and the callback is called multiple times inside an epoch, and the filename remains\n",
      " |          unchanged, the name of the saved file will be appended with a version count starting with ``v1`` to avoid\n",
      " |          collisions unless ``enable_version_counter`` is set to False. The version counter is unrelated to the top-k\n",
      " |          ranking of the checkpoint, and we recommend formatting the filename to include the monitored metric to avoid\n",
      " |          collisions.\n",
      " |      save_on_exception: Whether to save a checkpoint when an exception is raised. Default: ``False``.\n",
      " |      mode: one of {min, max}.\n",
      " |          If ``save_top_k != 0``, the decision to overwrite the current save file is made\n",
      " |          based on either the maximization or the minimization of the monitored quantity.\n",
      " |          For ``'val_acc'``, this should be ``'max'``, for ``'val_loss'`` this should be ``'min'``, etc.\n",
      " |      auto_insert_metric_name: When ``True``, the checkpoints filenames will contain the metric name.\n",
      " |          For example, ``filename='checkpoint_{epoch:02d}-{acc:02.0f}`` with epoch ``1`` and acc ``1.12`` will resolve\n",
      " |          to ``checkpoint_epoch=01-acc=01.ckpt``. Is useful to set it to ``False`` when metric names contain ``/``\n",
      " |          as this will result in extra folders.\n",
      " |          For example, ``filename='epoch={epoch}-step={step}-val_acc={val/acc:.2f}', auto_insert_metric_name=False``\n",
      " |      save_weights_only: if ``True``, then only the model's weights will be\n",
      " |          saved. Otherwise, the optimizer states, lr-scheduler states, etc are added in the checkpoint too.\n",
      " |      every_n_train_steps: How many training steps to wait before saving a checkpoint. This does not take into account\n",
      " |          the steps of the current epoch. If ``every_n_train_steps == None or every_n_train_steps == 0``,\n",
      " |          no checkpoints\n",
      " |          will be saved during training. Mutually exclusive with ``train_time_interval`` and ``every_n_epochs``.\n",
      " |\n",
      " |          .. note::\n",
      " |              When using with manual optimization, the checkpoint will be saved after the optimizer step by default.\n",
      " |              To save the model state before the optimizer step, you need to save the model state in your\n",
      " |              ``training_step`` before calling ``optimizer.step()``. See the class docstring for an example.\n",
      " |      train_time_interval: Checkpoints are monitored at the specified time interval.\n",
      " |          For all practical purposes, this cannot be smaller than the amount\n",
      " |          of time it takes to process a single training batch. This is not\n",
      " |          guaranteed to execute at the exact time specified, but should be close.\n",
      " |          This must be mutually exclusive with ``every_n_train_steps`` and ``every_n_epochs``.\n",
      " |      every_n_epochs: Number of epochs between checkpoints.\n",
      " |          This value must be ``None`` or non-negative.\n",
      " |          To disable saving top-k checkpoints, set ``every_n_epochs = 0``.\n",
      " |          This argument does not impact the saving of ``save_last=True`` checkpoints.\n",
      " |          If all of ``every_n_epochs``, ``every_n_train_steps`` and\n",
      " |          ``train_time_interval`` are ``None``, we save a checkpoint at the end of every epoch\n",
      " |          (equivalent to ``every_n_epochs = 1``).\n",
      " |          If ``every_n_epochs == None`` and either ``every_n_train_steps != None`` or ``train_time_interval != None``,\n",
      " |          saving at the end of each epoch is disabled\n",
      " |          (equivalent to ``every_n_epochs = 0``).\n",
      " |          This must be mutually exclusive with ``every_n_train_steps`` and ``train_time_interval``.\n",
      " |          Setting both ``ModelCheckpoint(..., every_n_epochs=V, save_on_train_epoch_end=False)`` and\n",
      " |          ``Trainer(max_epochs=N, check_val_every_n_epoch=M)``\n",
      " |          will only save checkpoints at epochs 0 < E <= N\n",
      " |          where both values for ``every_n_epochs`` and ``check_val_every_n_epoch`` evenly divide E.\n",
      " |      save_on_train_epoch_end: Whether to run checkpointing at the end of the training epoch.\n",
      " |          If ``True``, checkpoints are saved at the end of every training epoch.\n",
      " |          If ``False``, checkpoints are saved at the end of validation.\n",
      " |          If ``None`` (default), checkpointing behavior is determined based on training configuration.\n",
      " |          If ``val_check_interval`` is a str, dict, or `timedelta` (time-based), checkpointing is performed after\n",
      " |          validation.\n",
      " |          If ``check_val_every_n_epoch != 1``, checkpointing will not be performed at the end of\n",
      " |          every training epoch. If there are no validation batches of data, checkpointing will occur at the\n",
      " |          end of the training epoch. If there is a non-default number of validation runs per training epoch\n",
      " |          (``val_check_interval != 1``), checkpointing is performed after validation.\n",
      " |      enable_version_counter: Whether to append a version to the existing file name.\n",
      " |          If ``False``, then the checkpoint files will be overwritten.\n",
      " |\n",
      " |  Note:\n",
      " |      For extra customization, ModelCheckpoint includes the following attributes:\n",
      " |\n",
      " |      - ``CHECKPOINT_JOIN_CHAR = \"-\"``\n",
      " |      - ``CHECKPOINT_EQUALS_CHAR = \"=\"``\n",
      " |      - ``CHECKPOINT_NAME_LAST = \"last\"``\n",
      " |      - ``FILE_EXTENSION = \".ckpt\"``\n",
      " |      - ``STARTING_VERSION = 1``\n",
      " |\n",
      " |      For example, you can change the default last checkpoint name by doing\n",
      " |      ``checkpoint_callback.CHECKPOINT_NAME_LAST = \"{epoch}-last\"``\n",
      " |\n",
      " |      If you want to checkpoint every N hours, every M train batches, and/or every K val epochs,\n",
      " |      then you should create multiple ``ModelCheckpoint`` callbacks.\n",
      " |\n",
      " |      If the checkpoint's ``dirpath`` changed from what it was before while resuming the training,\n",
      " |      only ``best_model_path`` will be reloaded and a warning will be issued.\n",
      " |\n",
      " |      If you provide a ``filename`` on a mounted device where changing permissions is not allowed (causing ``chmod``\n",
      " |      to raise a ``PermissionError``), install `fsspec>=2025.5.0`. Then the error is caught, the file's permissions\n",
      " |      remain unchanged, and the checkpoint is still saved. Otherwise, no checkpoint will be saved and training stops.\n",
      " |\n",
      " |  Raises:\n",
      " |      MisconfigurationException:\n",
      " |          If ``save_top_k`` is smaller than ``-1``,\n",
      " |          if ``monitor`` is ``None`` and ``save_top_k`` is none of ``None``, ``-1``, and ``0``, or\n",
      " |          if ``mode`` is none of ``\"min\"`` or ``\"max\"``.\n",
      " |      ValueError:\n",
      " |          If ``trainer.save_checkpoint`` is ``None``.\n",
      " |\n",
      " |  Example::\n",
      " |\n",
      " |      >>> from lightning.pytorch import Trainer\n",
      " |      >>> from lightning.pytorch.callbacks import ModelCheckpoint\n",
      " |\n",
      " |      # saves checkpoints to 'my/path/' at every epoch\n",
      " |      >>> checkpoint_callback = ModelCheckpoint(dirpath='my/path/')\n",
      " |      >>> trainer = Trainer(callbacks=[checkpoint_callback])\n",
      " |\n",
      " |      # save epoch and val_loss in name\n",
      " |      # saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt\n",
      " |      >>> checkpoint_callback = ModelCheckpoint(\n",
      " |      ...     monitor='val_loss',\n",
      " |      ...     dirpath='my/path/',\n",
      " |      ...     filename='sample-mnist-{epoch:02d}-{val_loss:.2f}'\n",
      " |      ... )\n",
      " |\n",
      " |      # save epoch and val_loss in name, but specify the formatting yourself (e.g. to avoid problems with Tensorboard\n",
      " |      # or Neptune, due to the presence of characters like '=' or '/')\n",
      " |      # saves a file like: my/path/sample-mnist-epoch02-val_loss0.32.ckpt\n",
      " |      >>> checkpoint_callback = ModelCheckpoint(\n",
      " |      ...     monitor='val/loss',\n",
      " |      ...     dirpath='my/path/',\n",
      " |      ...     filename='sample-mnist-epoch{epoch:02d}-val_loss{val/loss:.2f}',\n",
      " |      ...     auto_insert_metric_name=False\n",
      " |      ... )\n",
      " |\n",
      " |      # retrieve the best checkpoint after training\n",
      " |      >>> checkpoint_callback = ModelCheckpoint(dirpath='my/path/')\n",
      " |      >>> trainer = Trainer(callbacks=[checkpoint_callback])\n",
      " |      >>> model = ...  # doctest: +SKIP\n",
      " |      >>> trainer.fit(model)  # doctest: +SKIP\n",
      " |      >>> print(checkpoint_callback.best_model_path)  # doctest: +SKIP\n",
      " |\n",
      " |  .. tip:: Saving and restoring multiple checkpoint callbacks at the same time is supported under variation in the\n",
      " |      following arguments:\n",
      " |\n",
      " |      *monitor, mode, every_n_train_steps, every_n_epochs, train_time_interval*\n",
      " |\n",
      " |      Read more: :ref:`Persisting Callback State <extensions/callbacks_state:save callback state>`\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      ModelCheckpoint\n",
      " |      lightning.pytorch.callbacks.checkpoint.Checkpoint\n",
      " |      lightning.pytorch.callbacks.callback.Callback\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(\n",
      " |      self,\n",
      " |      dirpath: Union[str, pathlib._local.Path, NoneType] = None,\n",
      " |      filename: Optional[str] = None,\n",
      " |      monitor: Optional[str] = None,\n",
      " |      verbose: bool = False,\n",
      " |      save_last: Union[bool, Literal['link'], NoneType] = None,\n",
      " |      save_top_k: int = 1,\n",
      " |      save_on_exception: bool = False,\n",
      " |      save_weights_only: bool = False,\n",
      " |      mode: str = 'min',\n",
      " |      auto_insert_metric_name: bool = True,\n",
      " |      every_n_train_steps: Optional[int] = None,\n",
      " |      train_time_interval: Optional[datetime.timedelta] = None,\n",
      " |      every_n_epochs: Optional[int] = None,\n",
      " |      save_on_train_epoch_end: Optional[bool] = None,\n",
      " |      enable_version_counter: bool = True\n",
      " |  )\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  check_monitor_top_k(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      current: Optional[torch.Tensor] = None\n",
      " |  ) -> bool\n",
      " |\n",
      " |  file_exists(\n",
      " |      self,\n",
      " |      filepath: Union[str, pathlib._local.Path],\n",
      " |      trainer: 'pl.Trainer'\n",
      " |  ) -> bool\n",
      " |      Checks if a file exists on rank 0 and synchronizes the result to all other ranks, preventing the internal\n",
      " |      state to diverge between ranks.\n",
      " |\n",
      " |  format_checkpoint_name(\n",
      " |      self,\n",
      " |      metrics: dict[str, torch.Tensor],\n",
      " |      filename: Optional[str] = None,\n",
      " |      ver: Optional[int] = None,\n",
      " |      prefix: Optional[str] = None\n",
      " |  ) -> str\n",
      " |      Generate a filename according to the defined template.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> tmpdir = os.path.dirname(__file__)\n",
      " |          >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}')\n",
      " |          >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=0)))\n",
      " |          'epoch=0.ckpt'\n",
      " |          >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch:03d}')\n",
      " |          >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=5)))\n",
      " |          'epoch=005.ckpt'\n",
      " |          >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}-{val_loss:.2f}')\n",
      " |          >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\n",
      " |          'epoch=2-val_loss=0.12.ckpt'\n",
      " |          >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.12), filename='{epoch:d}'))\n",
      " |          'epoch=2.ckpt'\n",
      " |          >>> ckpt = ModelCheckpoint(dirpath=tmpdir,\n",
      " |          ... filename='epoch={epoch}-validation_loss={val_loss:.2f}',\n",
      " |          ... auto_insert_metric_name=False)\n",
      " |          >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\n",
      " |          'epoch=2-validation_loss=0.12.ckpt'\n",
      " |          >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{missing:d}')\n",
      " |          >>> os.path.basename(ckpt.format_checkpoint_name({}))\n",
      " |          'missing=0.ckpt'\n",
      " |          >>> ckpt = ModelCheckpoint(filename='{step}')\n",
      " |          >>> os.path.basename(ckpt.format_checkpoint_name(dict(step=0)))\n",
      " |          'step=0.ckpt'\n",
      " |\n",
      " |  load_state_dict(self, state_dict: dict[str, typing.Any]) -> None\n",
      " |      Called when loading a checkpoint, implement to reload callback state given callback's ``state_dict``.\n",
      " |\n",
      " |      Args:\n",
      " |          state_dict: the callback state returned by ``state_dict``.\n",
      " |\n",
      " |  on_exception(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      exception: BaseException\n",
      " |  ) -> None\n",
      " |      Save a checkpoint when an exception is raised.\n",
      " |\n",
      " |  on_train_batch_end(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      outputs: Union[torch.Tensor, collections.abc.Mapping[str, Any], NoneType],\n",
      " |      batch: Any,\n",
      " |      batch_idx: int\n",
      " |  ) -> None\n",
      " |      Save checkpoint on train batch end if we meet the criteria for `every_n_train_steps`\n",
      " |\n",
      " |  on_train_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Ensure save_last=True is applied when training ends.\n",
      " |\n",
      " |  on_train_epoch_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Save a checkpoint at the end of the training epoch.\n",
      " |\n",
      " |  on_train_start(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when the train begins.\n",
      " |\n",
      " |  on_validation_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Save a checkpoint at the end of the validation stage.\n",
      " |\n",
      " |  setup(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule', stage: str) -> None\n",
      " |      Called when fit, validate, test, predict, or tune begins.\n",
      " |\n",
      " |  state_dict(self) -> dict[str, typing.Any]\n",
      " |      Called when saving a checkpoint, implement to generate callback's ``state_dict``.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary containing callback state.\n",
      " |\n",
      " |  to_yaml(self, filepath: Union[str, pathlib._local.Path, NoneType] = None) -> None\n",
      " |      Saves the `best_k_models` dict containing the checkpoint paths with the corresponding scores to a YAML\n",
      " |      file.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  every_n_epochs\n",
      " |\n",
      " |  state_key\n",
      " |      Identifier for the state of the callback.\n",
      " |\n",
      " |      Used to store and retrieve a callback's state from the checkpoint dictionary by\n",
      " |      ``checkpoint[\"callbacks\"][state_key]``. Implementations of a callback need to provide a unique state key if 1)\n",
      " |      the callback has state and 2) it is desired to maintain the state of multiple instances of that callback.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  CHECKPOINT_EQUALS_CHAR = '='\n",
      " |\n",
      " |  CHECKPOINT_JOIN_CHAR = '-'\n",
      " |\n",
      " |  CHECKPOINT_NAME_LAST = 'last'\n",
      " |\n",
      " |  FILE_EXTENSION = '.ckpt'\n",
      " |\n",
      " |  STARTING_VERSION = 1\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from lightning.pytorch.callbacks.callback.Callback:\n",
      " |\n",
      " |  on_after_backward(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called after ``loss.backward()`` and before optimizers are stepped.\n",
      " |\n",
      " |  on_before_backward(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      loss: torch.Tensor\n",
      " |  ) -> None\n",
      " |      Called before ``loss.backward()``.\n",
      " |\n",
      " |  on_before_optimizer_step(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      optimizer: torch.optim.optimizer.Optimizer\n",
      " |  ) -> None\n",
      " |      Called before ``optimizer.step()``.\n",
      " |\n",
      " |  on_before_zero_grad(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      optimizer: torch.optim.optimizer.Optimizer\n",
      " |  ) -> None\n",
      " |      Called before ``optimizer.zero_grad()``.\n",
      " |\n",
      " |  on_fit_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when fit ends.\n",
      " |\n",
      " |  on_fit_start(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when fit begins.\n",
      " |\n",
      " |  on_load_checkpoint(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      checkpoint: dict[str, typing.Any]\n",
      " |  ) -> None\n",
      " |      Called when loading a model checkpoint, use to reload state.\n",
      " |\n",
      " |      Args:\n",
      " |          trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n",
      " |          pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\n",
      " |          checkpoint: the full checkpoint dictionary that got loaded by the Trainer.\n",
      " |\n",
      " |  on_predict_batch_end(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      outputs: Any,\n",
      " |      batch: Any,\n",
      " |      batch_idx: int,\n",
      " |      dataloader_idx: int = 0\n",
      " |  ) -> None\n",
      " |      Called when the predict batch ends.\n",
      " |\n",
      " |  on_predict_batch_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      batch: Any,\n",
      " |      batch_idx: int,\n",
      " |      dataloader_idx: int = 0\n",
      " |  ) -> None\n",
      " |      Called when the predict batch begins.\n",
      " |\n",
      " |  on_predict_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when predict ends.\n",
      " |\n",
      " |  on_predict_epoch_end(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the predict epoch ends.\n",
      " |\n",
      " |  on_predict_epoch_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the predict epoch begins.\n",
      " |\n",
      " |  on_predict_start(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when the predict begins.\n",
      " |\n",
      " |  on_sanity_check_end(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the validation sanity check ends.\n",
      " |\n",
      " |  on_sanity_check_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the validation sanity check starts.\n",
      " |\n",
      " |  on_save_checkpoint(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      checkpoint: dict[str, typing.Any]\n",
      " |  ) -> None\n",
      " |      Called when saving a checkpoint to give you a chance to store anything else you might want to save.\n",
      " |\n",
      " |      Args:\n",
      " |          trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n",
      " |          pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\n",
      " |          checkpoint: the checkpoint dictionary that will be saved.\n",
      " |\n",
      " |  on_test_batch_end(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      outputs: Union[torch.Tensor, collections.abc.Mapping[str, Any], NoneType],\n",
      " |      batch: Any,\n",
      " |      batch_idx: int,\n",
      " |      dataloader_idx: int = 0\n",
      " |  ) -> None\n",
      " |      Called when the test batch ends.\n",
      " |\n",
      " |  on_test_batch_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      batch: Any,\n",
      " |      batch_idx: int,\n",
      " |      dataloader_idx: int = 0\n",
      " |  ) -> None\n",
      " |      Called when the test batch begins.\n",
      " |\n",
      " |  on_test_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when the test ends.\n",
      " |\n",
      " |  on_test_epoch_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when the test epoch ends.\n",
      " |\n",
      " |  on_test_epoch_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the test epoch begins.\n",
      " |\n",
      " |  on_test_start(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when the test begins.\n",
      " |\n",
      " |  on_train_batch_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      batch: Any,\n",
      " |      batch_idx: int\n",
      " |  ) -> None\n",
      " |      Called when the train batch begins.\n",
      " |\n",
      " |  on_train_epoch_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the train epoch begins.\n",
      " |\n",
      " |  on_validation_batch_end(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      outputs: Union[torch.Tensor, collections.abc.Mapping[str, Any], NoneType],\n",
      " |      batch: Any,\n",
      " |      batch_idx: int,\n",
      " |      dataloader_idx: int = 0\n",
      " |  ) -> None\n",
      " |      Called when the validation batch ends.\n",
      " |\n",
      " |  on_validation_batch_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      batch: Any,\n",
      " |      batch_idx: int,\n",
      " |      dataloader_idx: int = 0\n",
      " |  ) -> None\n",
      " |      Called when the validation batch begins.\n",
      " |\n",
      " |  on_validation_epoch_end(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the val epoch ends.\n",
      " |\n",
      " |  on_validation_epoch_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the val epoch begins.\n",
      " |\n",
      " |  on_validation_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the validation loop begins.\n",
      " |\n",
      " |  teardown(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      stage: str\n",
      " |  ) -> None\n",
      " |      Called when fit, validate, test, predict, or tune ends.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from lightning.pytorch.callbacks.callback.Callback:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ModelCheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ceda94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class EarlyStopping in module lightning.pytorch.callbacks.early_stopping:\n",
      "\n",
      "class EarlyStopping(lightning.pytorch.callbacks.callback.Callback)\n",
      " |  EarlyStopping(\n",
      " |      monitor: str,\n",
      " |      min_delta: float = 0.0,\n",
      " |      patience: int = 3,\n",
      " |      verbose: bool = False,\n",
      " |      mode: str = 'min',\n",
      " |      strict: bool = True,\n",
      " |      check_finite: bool = True,\n",
      " |      stopping_threshold: Optional[float] = None,\n",
      " |      divergence_threshold: Optional[float] = None,\n",
      " |      check_on_train_epoch_end: Optional[bool] = None,\n",
      " |      log_rank_zero_only: bool = False\n",
      " |  )\n",
      " |\n",
      " |  Monitor a metric and stop training when it stops improving.\n",
      " |\n",
      " |  Args:\n",
      " |      monitor: quantity to be monitored.\n",
      " |      min_delta: minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute\n",
      " |          change of less than or equal to `min_delta`, will count as no improvement.\n",
      " |      patience: number of checks with no improvement\n",
      " |          after which training will be stopped. Under the default configuration, one check happens after\n",
      " |          every training epoch. However, the frequency of validation can be modified by setting various parameters on\n",
      " |          the ``Trainer``, for example ``check_val_every_n_epoch`` and ``val_check_interval``.\n",
      " |\n",
      " |          .. note::\n",
      " |\n",
      " |              It must be noted that the patience parameter counts the number of validation checks with\n",
      " |              no improvement, and not the number of training epochs. Therefore, with parameters\n",
      " |              ``check_val_every_n_epoch=10`` and ``patience=3``, the trainer will perform at least 40 training\n",
      " |              epochs before being stopped.\n",
      " |\n",
      " |      verbose: verbosity mode.\n",
      " |      mode: one of ``'min'``, ``'max'``. In ``'min'`` mode, training will stop when the quantity\n",
      " |          monitored has stopped decreasing and in ``'max'`` mode it will stop when the quantity\n",
      " |          monitored has stopped increasing.\n",
      " |      strict: whether to crash the training if `monitor` is not found in the validation metrics.\n",
      " |      check_finite: When set ``True``, stops training when the monitor becomes NaN or infinite.\n",
      " |      stopping_threshold: Stop training immediately once the monitored quantity reaches this threshold.\n",
      " |      divergence_threshold: Stop training as soon as the monitored quantity becomes worse than this threshold.\n",
      " |      check_on_train_epoch_end: whether to run early stopping at the end of the training epoch.\n",
      " |          If this is ``False``, then the check runs at the end of the validation.\n",
      " |      log_rank_zero_only: When set ``True``, logs the status of the early stopping callback only for rank 0 process.\n",
      " |\n",
      " |  Attributes:\n",
      " |      stopped_epoch: The epoch at which training was stopped. 0 if training was not stopped.\n",
      " |      stopping_reason: An ``EarlyStoppingReason`` enum indicating why training was stopped.\n",
      " |      stopping_reason_message: A human-readable message explaining why training was stopped.\n",
      " |\n",
      " |  Raises:\n",
      " |      MisconfigurationException:\n",
      " |          If ``mode`` is none of ``\"min\"`` or ``\"max\"``.\n",
      " |      RuntimeError:\n",
      " |          If the metric ``monitor`` is not available.\n",
      " |\n",
      " |  Example::\n",
      " |\n",
      " |      >>> from lightning.pytorch import Trainer\n",
      " |      >>> from lightning.pytorch.callbacks import EarlyStopping\n",
      " |      >>> from lightning.pytorch.callbacks.early_stopping import EarlyStoppingReason\n",
      " |      >>> early_stopping = EarlyStopping('val_loss')\n",
      " |      >>> trainer = Trainer(callbacks=[early_stopping])\n",
      " |      >>> # After training...\n",
      " |      >>> if early_stopping.stopping_reason == EarlyStoppingReason.PATIENCE_EXHAUSTED:\n",
      " |      ...     print(\"Training stopped due to patience exhaustion\")\n",
      " |\n",
      " |  .. tip:: Saving and restoring multiple early stopping callbacks at the same time is supported under variation in the\n",
      " |      following arguments:\n",
      " |\n",
      " |      *monitor, mode*\n",
      " |\n",
      " |      Read more: :ref:`Persisting Callback State <extensions/callbacks_state:save callback state>`\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      EarlyStopping\n",
      " |      lightning.pytorch.callbacks.callback.Callback\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(\n",
      " |      self,\n",
      " |      monitor: str,\n",
      " |      min_delta: float = 0.0,\n",
      " |      patience: int = 3,\n",
      " |      verbose: bool = False,\n",
      " |      mode: str = 'min',\n",
      " |      strict: bool = True,\n",
      " |      check_finite: bool = True,\n",
      " |      stopping_threshold: Optional[float] = None,\n",
      " |      divergence_threshold: Optional[float] = None,\n",
      " |      check_on_train_epoch_end: Optional[bool] = None,\n",
      " |      log_rank_zero_only: bool = False\n",
      " |  )\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  load_state_dict(self, state_dict: dict[str, typing.Any]) -> None\n",
      " |      Called when loading a checkpoint, implement to reload callback state given callback's ``state_dict``.\n",
      " |\n",
      " |      Args:\n",
      " |          state_dict: the callback state returned by ``state_dict``.\n",
      " |\n",
      " |  on_train_epoch_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when the train epoch ends.\n",
      " |\n",
      " |      To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the\n",
      " |      :class:`lightning.pytorch.core.LightningModule` and access them in this hook:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          class MyLightningModule(L.LightningModule):\n",
      " |              def __init__(self):\n",
      " |                  super().__init__()\n",
      " |                  self.training_step_outputs = []\n",
      " |\n",
      " |              def training_step(self):\n",
      " |                  loss = ...\n",
      " |                  self.training_step_outputs.append(loss)\n",
      " |                  return loss\n",
      " |\n",
      " |\n",
      " |          class MyCallback(L.Callback):\n",
      " |              def on_train_epoch_end(self, trainer, pl_module):\n",
      " |                  # do something with all training_step outputs, for example:\n",
      " |                  epoch_mean = torch.stack(pl_module.training_step_outputs).mean()\n",
      " |                  pl_module.log(\"training_epoch_mean\", epoch_mean)\n",
      " |                  # free up the memory\n",
      " |                  pl_module.training_step_outputs.clear()\n",
      " |\n",
      " |  on_validation_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when the validation loop ends.\n",
      " |\n",
      " |  setup(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule', stage: str) -> None\n",
      " |      Called when fit, validate, test, predict, or tune begins.\n",
      " |\n",
      " |  state_dict(self) -> dict[str, typing.Any]\n",
      " |      Called when saving a checkpoint, implement to generate callback's ``state_dict``.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary containing callback state.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  monitor_op\n",
      " |\n",
      " |  state_key\n",
      " |      Identifier for the state of the callback.\n",
      " |\n",
      " |      Used to store and retrieve a callback's state from the checkpoint dictionary by\n",
      " |      ``checkpoint[\"callbacks\"][state_key]``. Implementations of a callback need to provide a unique state key if 1)\n",
      " |      the callback has state and 2) it is desired to maintain the state of multiple instances of that callback.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  mode_dict = {'max': <built-in method gt of type object>, 'min': <built...\n",
      " |\n",
      " |  order_dict = {'max': '>', 'min': '<'}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from lightning.pytorch.callbacks.callback.Callback:\n",
      " |\n",
      " |  on_after_backward(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called after ``loss.backward()`` and before optimizers are stepped.\n",
      " |\n",
      " |  on_before_backward(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      loss: torch.Tensor\n",
      " |  ) -> None\n",
      " |      Called before ``loss.backward()``.\n",
      " |\n",
      " |  on_before_optimizer_step(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      optimizer: torch.optim.optimizer.Optimizer\n",
      " |  ) -> None\n",
      " |      Called before ``optimizer.step()``.\n",
      " |\n",
      " |  on_before_zero_grad(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      optimizer: torch.optim.optimizer.Optimizer\n",
      " |  ) -> None\n",
      " |      Called before ``optimizer.zero_grad()``.\n",
      " |\n",
      " |  on_exception(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      exception: BaseException\n",
      " |  ) -> None\n",
      " |      Called when any trainer execution is interrupted by an exception.\n",
      " |\n",
      " |  on_fit_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when fit ends.\n",
      " |\n",
      " |  on_fit_start(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when fit begins.\n",
      " |\n",
      " |  on_load_checkpoint(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      checkpoint: dict[str, typing.Any]\n",
      " |  ) -> None\n",
      " |      Called when loading a model checkpoint, use to reload state.\n",
      " |\n",
      " |      Args:\n",
      " |          trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n",
      " |          pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\n",
      " |          checkpoint: the full checkpoint dictionary that got loaded by the Trainer.\n",
      " |\n",
      " |  on_predict_batch_end(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      outputs: Any,\n",
      " |      batch: Any,\n",
      " |      batch_idx: int,\n",
      " |      dataloader_idx: int = 0\n",
      " |  ) -> None\n",
      " |      Called when the predict batch ends.\n",
      " |\n",
      " |  on_predict_batch_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      batch: Any,\n",
      " |      batch_idx: int,\n",
      " |      dataloader_idx: int = 0\n",
      " |  ) -> None\n",
      " |      Called when the predict batch begins.\n",
      " |\n",
      " |  on_predict_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when predict ends.\n",
      " |\n",
      " |  on_predict_epoch_end(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the predict epoch ends.\n",
      " |\n",
      " |  on_predict_epoch_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the predict epoch begins.\n",
      " |\n",
      " |  on_predict_start(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when the predict begins.\n",
      " |\n",
      " |  on_sanity_check_end(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the validation sanity check ends.\n",
      " |\n",
      " |  on_sanity_check_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the validation sanity check starts.\n",
      " |\n",
      " |  on_save_checkpoint(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      checkpoint: dict[str, typing.Any]\n",
      " |  ) -> None\n",
      " |      Called when saving a checkpoint to give you a chance to store anything else you might want to save.\n",
      " |\n",
      " |      Args:\n",
      " |          trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n",
      " |          pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\n",
      " |          checkpoint: the checkpoint dictionary that will be saved.\n",
      " |\n",
      " |  on_test_batch_end(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      outputs: Union[torch.Tensor, collections.abc.Mapping[str, Any], NoneType],\n",
      " |      batch: Any,\n",
      " |      batch_idx: int,\n",
      " |      dataloader_idx: int = 0\n",
      " |  ) -> None\n",
      " |      Called when the test batch ends.\n",
      " |\n",
      " |  on_test_batch_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      batch: Any,\n",
      " |      batch_idx: int,\n",
      " |      dataloader_idx: int = 0\n",
      " |  ) -> None\n",
      " |      Called when the test batch begins.\n",
      " |\n",
      " |  on_test_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when the test ends.\n",
      " |\n",
      " |  on_test_epoch_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when the test epoch ends.\n",
      " |\n",
      " |  on_test_epoch_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the test epoch begins.\n",
      " |\n",
      " |  on_test_start(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when the test begins.\n",
      " |\n",
      " |  on_train_batch_end(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      outputs: Union[torch.Tensor, collections.abc.Mapping[str, Any], NoneType],\n",
      " |      batch: Any,\n",
      " |      batch_idx: int\n",
      " |  ) -> None\n",
      " |      Called when the train batch ends.\n",
      " |\n",
      " |      Note:\n",
      " |          The value ``outputs[\"loss\"]`` here will be the normalized value w.r.t ``accumulate_grad_batches`` of the\n",
      " |          loss returned from ``training_step``.\n",
      " |\n",
      " |  on_train_batch_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      batch: Any,\n",
      " |      batch_idx: int\n",
      " |  ) -> None\n",
      " |      Called when the train batch begins.\n",
      " |\n",
      " |  on_train_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when the train ends.\n",
      " |\n",
      " |  on_train_epoch_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the train epoch begins.\n",
      " |\n",
      " |  on_train_start(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None\n",
      " |      Called when the train begins.\n",
      " |\n",
      " |  on_validation_batch_end(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      outputs: Union[torch.Tensor, collections.abc.Mapping[str, Any], NoneType],\n",
      " |      batch: Any,\n",
      " |      batch_idx: int,\n",
      " |      dataloader_idx: int = 0\n",
      " |  ) -> None\n",
      " |      Called when the validation batch ends.\n",
      " |\n",
      " |  on_validation_batch_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      batch: Any,\n",
      " |      batch_idx: int,\n",
      " |      dataloader_idx: int = 0\n",
      " |  ) -> None\n",
      " |      Called when the validation batch begins.\n",
      " |\n",
      " |  on_validation_epoch_end(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the val epoch ends.\n",
      " |\n",
      " |  on_validation_epoch_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the val epoch begins.\n",
      " |\n",
      " |  on_validation_start(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule'\n",
      " |  ) -> None\n",
      " |      Called when the validation loop begins.\n",
      " |\n",
      " |  teardown(\n",
      " |      self,\n",
      " |      trainer: 'pl.Trainer',\n",
      " |      pl_module: 'pl.LightningModule',\n",
      " |      stage: str\n",
      " |  ) -> None\n",
      " |      Called when fit, validate, test, predict, or tune ends.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from lightning.pytorch.callbacks.callback.Callback:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(EarlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dff470c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\n",
    "    save_dir='logs',\n",
    "    name='autoencoder_mnist',\n",
    "    version=None,\n",
    "    prefix='test_'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(logger.log_dir, \"checkpoints\"),\n",
    "    filename=\"autoencoder_best-{epoch:02d}-{val_loss:.3f}\",\n",
    "    monitor=\"val_loss\",    \n",
    "    mode=\"min\",\n",
    "    save_top_k=3,     # keep ONLY the best\n",
    "    save_last=True    # ALSO save last.ckpt\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "beb755af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitAutoEncoder(Encoder(), Decoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7be1ff9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc198f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA L40S') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name    | Type    | Params | Mode  | FLOPs\n",
      "----------------------------------------------------\n",
      "0 | encoder | Encoder | 54.7 K | train | 0    \n",
      "1 | decoder | Decoder | 55.4 K | train | 0    \n",
      "----------------------------------------------------\n",
      "110 K     Trainable params\n",
      "0         Non-trainable params\n",
      "110 K     Total params\n",
      "0.440     Total estimated model params size (MB)\n",
      "14        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  20%|        | 75/375 [00:00<00:02, 144.99it/s, v_num=1, val_loss=0.0647, train_loss=0.0661] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks.early_stopping import EarlyStoppingReason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001bf20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check why training stopped\n",
    "if early_stop_callback.stopping_reason == EarlyStoppingReason.PATIENCE_EXHAUSTED:\n",
    "    print(\"Training stopped due to patience exhaustion\")\n",
    "elif early_stop_callback.stopping_reason == EarlyStoppingReason.STOPPING_THRESHOLD:\n",
    "    print(\"Training stopped due to reaching stopping threshold\")\n",
    "elif early_stop_callback.stopping_reason == EarlyStoppingReason.NOT_STOPPED:\n",
    "    print(\"Training completed normally without early stopping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access human-readable message\n",
    "if early_stop_callback.stopping_reason_message:\n",
    "    print(f\"Details: {early_stop_callback.stopping_reason_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc07f81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
